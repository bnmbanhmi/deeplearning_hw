{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bnmbanhmi/deeplearning_hw/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsDbolSFftXR"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Assuming you have a `segmentation_models` folder with the necessary model definitions\n",
    "from segmentation_models import UnetPlusPlus\n",
    "\n",
    "# Assuming you have a `config.py` file with configurations\n",
    "from config import *\n",
    "\n",
    "# Assuming you have a `dataset.py` file with data loading and preprocessing functions\n",
    "from dataset import load_data, read_image, show_example, read_mask, convert2TfDataset\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, img_dir, label_dir, resize=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        lower1 = np.array([0, 100, 20])\n",
    "        upper1 = np.array([10, 255, 255])\n",
    "\n",
    "        lower2 = np.array([160, 100, 20])\n",
    "        upper2 = np.array([179, 255, 255])\n",
    "        lower_mask = cv2.inRange(image, lower1, upper1)\n",
    "        upper_mask = cv2.inRange(image, lower2, upper2)\n",
    "\n",
    "        red_mask = lower_mask + upper_mask\n",
    "        red_mask[red_mask != 0] = 1\n",
    "\n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2\n",
    "\n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = np.expand_dims(full_mask, axis=-1)\n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "\n",
    "        return full_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.read_mask(label_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def show_image(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        image = plt.imread(img_path)\n",
    "        label = plt.imread(label_path)\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axs[0].imshow(image)\n",
    "        axs[0].set_title('Image')\n",
    "        axs[1].imshow(label)\n",
    "        axs[1].set_title('Label')\n",
    "        plt.show()\n",
    "\n",
    "# Load the dataset\n",
    "images, masks = load_data(IMAGE_PATH, MASK_PATH)\n",
    "print(f'Amount of images: {len(images)}')\n",
    "\n",
    "# Split the dataset\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "print(f'Training: {len(train_x)} - Validation: {len(valid_x)}')\n",
    "\n",
    "# Calculate steps\n",
    "train_step = len(train_x) // BATCH_SIZE\n",
    "if len(train_x) % BATCH_SIZE != 0:\n",
    "    train_step += 1\n",
    "valid_step = len(valid_x) // BATCH_SIZE\n",
    "if len(valid_x) % BATCH_SIZE != BATCH_SIZE:\n",
    "    valid_step += 1\n",
    "print(f'{train_step} - {valid_step}')\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = convert2TfDataset(train_x, train_y, BATCH_SIZE)\n",
    "valid_dataset = convert2TfDataset(valid_x, valid_y, BATCH_SIZE)\n",
    "\n",
    "# Initialize wandb\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"my-awesome-project\", config={\"batch_size\": BATCH_SIZE, \"epochs\": 20})\n",
    "\n",
    "# Build the model\n",
    "model = UnetPlusPlus(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=3\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint('best_model.h5', verbose=1, save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-6),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    WandbCallback()\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "H = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    steps_per_epoch=train_step,\n",
    "    validation_steps=valid_step,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot metrics\n",
    "fig = plt.figure()\n",
    "numOfEpoch = 20\n",
    "plt.plot(np.arange(0, numOfEpoch), H.history['loss'], label='training loss')\n",
    "plt.plot(np.arange(0, numOfEpoch), H.history['val_loss'], label='validation loss')\n",
    "plt.plot(np.arange(0, numOfEpoch), H.history['acc'], label='accuracy')\n",
    "plt.plot(np.arange(0, numOfEpoch), H.history['val_acc'], label='validation accuracy')\n",
    "plt.title('Accuracy and Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss|Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCW24B3Qmp6e54WjGYW6F9",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
